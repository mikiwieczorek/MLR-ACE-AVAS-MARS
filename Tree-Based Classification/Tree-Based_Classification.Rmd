---
title: "Tree-Based_Classification"
author: "Samuel A, Mikolaj W"
date: "April 17, 2020"
output: word_document
---

PROBLEM 1 ––  SATELLITE IMAGE DATA
The goal here is to predict the type of ground cover from a satellite image broken up into pixels.
Description from UCI Machine Learning database:
The database consists of the multi-spectral values of pixels in 3x3 neighborhoods in a satellite image, and the classification associated with the central pixel in each neighborhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. 

The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterized by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach. 

One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. 

The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighborhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighborhood and a number indicating the classification label of the central pixel. The number is a code for the following classes: 

Number Class 
1 red soil 
2 cotton crop 
3 grey soil 
4 damp grey soil 
5 soil with vegetation stubble 
6 mixture class (all types present)  
7 very damp grey soil 

Note: There are no examples with class 6 in this dataset. 

The data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset. 

In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. 



a) Compare CART/RPART, bagged CART/RPART, Random Forest classification, and Boosted 
     Trees in the classification of the test cases.   Which method performs best for these data?  Be 
     sure to adjust the various tuning parameters to optimize the performance of these methods 
     for this prediction problem.  Show the model development process for each of these methods 
     and report the final settings you used for any tuning parameters.  (20 pts. – 5 pts. for each method)


Getting started:

```{r}
setwd(getwd())
SATimage = read.csv("SATimage.csv")
SATimage = data.frame(class=as.factor(SATimage$class),SATimage[,1:36])


set.seed(888)
testcases = sample(1:dim(SATimage)[1],1000,replace=F)
SATtest = SATimage[testcases,]
SATtrain = SATimage[-testcases,]

```

Packages:
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
require(rpart)
require(rpart.plot)
require(ipred)
```


RPART basic Models:

Misclass Function:
```{r}
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
```




fitting a simple tree
```{r}
mod.default = rpart(class~.,data=SATtrain)
prp(mod.default)
```

```{r}
phat = predict(mod.default,newdata=SATtest,type="prob")
head(phat)
```


```{r}
yhat = predict(mod.default,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

Basic improvements

```{r}
control = rpart.control(minsplit=5,minbucket=3,cp=.001)
mod2 = rpart(class~.,data=SATtrain,control=control)
prp(mod2)
```

```{r}
yhat = predict(mod2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
crpart.sscv = function(fit,y,data,B=25,p=.333) {
n = length(y)
cv <- rep(0,B)
for (i in 1:B) {
       ss <- floor(n*p)
       sam <- sample(1:n,ss)
       temp <- data[-sam,]
       fit2 <- rpart(formula(fit),data=temp,parms=fit$parms,control=fit$control)
       ynew <- predict(fit2,newdata=data[sam,],type="class")
       tab <- table(y[sam],ynew)
       mc <- ss - sum(diag(tab))
      cv[i] <- mc/ss
      }
   cv
}
results = crpart.sscv(mod2,SATimage$class,data=SATimage,B=50)
summary(results)
```

While this second model is an improvement, we should still see if we can't push it further.
Let's walk it back and try a simpler model firdt with a greater requirement for splits and a larger bucket size.


```{r}
control = rpart.control(minsplit=7,minbucket=5,cp=.001)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```

```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
plotcp(mod2)
```


```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```

Very little changed, perhaps we need greater complexity. Perhaps by moving in the opposite direction with our metrics can find a better solution


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=.001)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```


```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```

Adding in just a little more complexity, we have inched out a bit more on our performance on our training set whilst being just as good as mod2 on the test set. The only metric left to truly optimize may just be the complexity parameter, so we will try next, alough this currently is the best model we have for basic rpart.

```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=.01)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```


```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```

While clearly less is not more, perhaps more is more to at least a certain extant.


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=.0005)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```


```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```

That was certainly a mistake. Perhaps we can find a better middle ground?

```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=.005)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```


```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```


Still a massive loss compared to others, one final adjustment will be made to attemtp an improvment.

```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=.002)
modx = rpart(class~.,data=SATtrain,control=control)
prp(modx)
```


```{r}
yhat = predict(modx,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

```{r}
plotcp(mod2)
```

```{r}
results = crpart.sscv(modx,SATimage$class,data=SATimage,B=50)
summary(results)
```

Still no improvement, it seems more and more likely that for any other further improvement we will have to change our whole methodology. Still, for how simply it is to implement and understand, a missclassification rate of about 13% isn't bad.


Final Model:

```{r}
# control = rpart.control(minsplit=3,minbucket=2,cp=.001)
# modx = rpart(class~.,data=SATtrain,control=control)
```

Metrics:

For Training (Mean): 0.1413

For Test: 0.134













RPART bagging Models:

Fit a basic model:
```{r}
sat.bag = bagging(class~.,data=SATtrain,coob=T)
sat.bag
```

check it:
```{r}
phat = predict(sat.bag,newdata=SATtest,type="prob")
head(phat)
```

```{r}
yhat = predict(sat.bag,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```
An improvement already, lets push it further.

```{r}
control = rpart.control(minsplit=5,minbucket=3,cp=0,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=100,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```

Split-Sample function
```{r}
bagg.sscv = function(fit,y,data,B=25,nbagg=100,p=.333) {
n = length(y)
cv <- rep(0,B)
for (i in 1:B) {
       ss <- floor(n*p)
       sam <- sample(1:n,ss)
       temp <- data[-sam,]
       fit2 <- bagging(formula(fit),data=temp,control=fit$control,coob=F)
       ynew <- predict(fit2,newdata=data[sam,],type="class")
       tab <- table(y[sam],ynew)
       mc <- ss - sum(diag(tab))
      cv[i] <- mc/ss
      }
   cv
}
```


Using it:
```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=25)
summary(results)
```

While this is better, it doesn't exactly blow us away in terms of succeeding over the normal bagging model. Lets see if we can optimize more.


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=0,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=100,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=25)
summary(results)

```


Using our bucket and split settings from the previous model made for better predictions on the test set, but worse on split-sample mean. While we personally would prefer this model, we are still hesistant to call it better. Maybe the complexity parameter can forward it.


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=0.001,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=100,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=25)
summary(results)

```

A slight decrease, maybe the bagging settings themselves we provide more.


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=0,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=150,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=150)
summary(results)

```

More bagging brought in a slight decrease in both rates whilst avoiding overfitting. But can we go farther?


```{r}
control = rpart.control(minsplit=3,minbucket=2,cp=0,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=200,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=200)
summary(results)

```

A minor improvement, although with metric like these we begin to fear overfitting. What was gained probably was not worth what was spent here. Lets try one more slight alteration before moving on to more advanced methods


```{r}
control = rpart.control(minsplit=4,minbucket=4,cp=0,xval=0)
sat.bag2 = bagging(class~.,data=SATtrain,nbagg=175,coob=T,control=control)
sat.bag2
```


```{r}
yhat = predict(sat.bag2,newdata=SATtest,type="class")
misclass(yhat,SATtest$class)
```


```{r}
results = bagg.sscv(mod2,SATimage$class,data=SATimage,B=5,nbagg=175)
summary(results)

```

While not inherently bad, it seems like we are too the point that no amount of adjusting is gonna gain us anything major in either category with out simply just overfitting the data. Instead, we should opt to instead use a more advanced method of classification, although looking at these models still has its merits.


Final Model Settings:

```{r}
# control = rpart.control(minsplit=3,minbucket=2,cp=0,xval=0)
# sat.bag2 = bagging(class~.,data=SATtrain,nbagg=150,coob=T,control=control)
```

Metrics:

Test Set: 0.098

Training Set (Mean):   0.1198


Though bagging was a clear improvment, random forest will probably out do it.


Random Forest:

Packages:
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
require(randomForest)
```


Fitting a basic model:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=1,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 1,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 1

        OOB estimate of  error rate: 9.99%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 801   1  15   0   3   0  0.02317073
2   2 362   1   2   4   3  0.03208556
3   5   0 714  15   0   5  0.03382950
4   4   3  65 183   3  65  0.43343653
5  30   1   1   6 298  32  0.19021739
7   0   0  13  48  16 734  0.09494451


Checking it:

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   1   7   0
  2   0 102   0   0   2   0
  3   2   0 212  19   1   6
  4   0   0   5  52   0   6
  5   3   1   0   0  83   3
  7   0   2   3  20   9 212


Misclassification Rate =  0.092


We already can see an improvment over our best bag model, although some internal split sample should still be considered.


Split-Sample function for Random Forest
```{r}
crf.sscv = function(fit,y,data,B=25,p=.333,mtry=fit$mtry,ntree=fit$ntree) {
n = length(y)
cv <- rep(0,B)
   for (i in 1:B) {
ss <- floor(n*p)
sam <- sample(1:n,ss)
temp <- data[-sam,]
fit2 <- randomForest(formula(fit),data=temp,mtry=mtry,ntree=ntree)
ynew <- predict(fit2,newdata=data[sam,],type="class")
tab <- table(y[sam],ynew)
mc <- ss - sum(diag(tab))
cv[i] <- mc/ss
}
   cv
}

```


Using it:
```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```

The improvment holds up here as well, however there are still many features we can try to adjust to pull the most out of this kind of model. Well start with finding the preffered value for mtry, or the number of variables randomly sampled at each split.






mtry = 2:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=2,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 2,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 9.29%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 804   1  13   0   2   0  0.01951220
2   1 365   0   2   3   3  0.02406417
3   3   2 717  10   0   7  0.02976996
4   4   4  64 192   2  57  0.40557276
5  26   2   1   3 302  34  0.17934783
7   0   1  15  48  11 736  0.09247842

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   2   5   0
  2   0 102   0   0   2   0
  3   3   0 212  19   0   6
  4   0   0   5  52   0   6
  5   2   1   0   0  86   4
  7   0   2   3  19   9 211


Misclassification Rate =  0.09

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08311 0.09449 0.09974 0.09928 0.10324 0.11636


mtry = 3:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=3,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 3,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 3

        OOB estimate of  error rate: 9.11%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 796   2  17   0   5   0  0.02926829
2   0 367   0   2   4   1  0.01871658
3   4   1 716  12   0   6  0.03112314
4   3   3  64 192   2  59  0.40557276
5  24   2   1   3 310  28  0.15760870
7   0   1  12  44  13 741  0.08631319


```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   2   5   0
  2   0 102   0   0   2   0
  3   3   0 212  19   0   5
  4   0   0   5  54   0   6
  5   2   1   0   0  86   4
  7   0   2   3  17   9 212


Misclassification Rate =  0.087

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08399 0.09274 0.09711 0.09788 0.10324 0.11549 


mtry = 4:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=4,importance=T)
# sat.rf

```

Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 4,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 4

        OOB estimate of  error rate: 9%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 801   2  13   0   4   0  0.02317073
2   0 364   1   2   4   3  0.02673797
3   3   1 716  12   0   7  0.03112314
4   3   3  66 198   2  51  0.38699690
5  24   2   0   6 311  25  0.15489130
7   0   1  13  49  12 736  0.09247842

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```

Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   2   5   0
  2   0 102   0   0   2   0
  3   3   0 212  19   0   5
  4   0   0   5  53   0   6
  5   2   1   0   0  87   3
  7   0   2   3  18   8 213


Misclassification Rate =  0.086


```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08049 0.09099 0.09624 0.09655 0.10149 0.11724 


mtry = 5:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=5,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 5,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 5

        OOB estimate of  error rate: 9.05%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 799   1  15   0   5   0  0.02560976
2   1 364   1   2   4   2  0.02673797
3   5   1 713  12   1   7  0.03518268
4   2   3  64 195   2  57  0.39628483
5  20   2   0   5 314  27  0.14673913
7   0   1  12  43  16 739  0.08877928


```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   2   2   5   0
  2   0 103   0   0   2   0
  3   3   0 212  18   0   5
  4   0   0   5  54   0   6
  5   3   0   0   0  85   4
  7   0   2   3  18  10 212


Misclassification Rate =  0.088

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08224 0.09536 0.10061 0.09963 0.10324 0.11811 


mtry = 6:
```{r}
sat.rf = randomForest(class~.,data=SATtrain,mtry=6,importance=T)
sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 6,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 6

        OOB estimate of  error rate: 8.85%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 799   2  15   0   4   0  0.02560976
2   0 366   0   2   4   2  0.02139037
3   3   1 714  14   0   7  0.03382950
4   4   3  63 198   2  53  0.38699690
5  21   2   0   4 316  25  0.14130435
7   0   1  12  42  18 738  0.09001233


```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   1   2   5   0
  2   0 102   0   0   2   0
  3   3   0 215  19   0   5
  4   0   0   5  52   0   6
  5   2   1   0   0  86   4
  7   0   2   1  19   9 212


Misclassification Rate =  0.086


```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08049 0.09274 0.09536 0.09596 0.10149 0.11374 

We can see that out of the gate we are seeing improvement as we consider more variables, knowing this, we will try and zero in on that target.


mtry = 7:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T)
# sat.rf

```

Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 8.68%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 800   2  14   0   4   0  0.02439024
2   1 366   0   2   3   2  0.02139037
3   2   1 714  15   0   7  0.03382950
4   4   2  64 193   2  58  0.40247678
5  18   3   0   4 318  25  0.13586957
7   0   1  14  37  13 746  0.08014797

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   1   2   5   0
  2   0 104   0   0   2   0
  3   3   0 214  19   0   5
  4   0   0   5  54   0   6
  5   3   0   0   0  85   3
  7   0   1   2  17  10 213


Misclassification Rate =  0.084

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08661 0.09274 0.09974 0.09876 0.10411 0.11199 



mtry = 10:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=10,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 10,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 10

        OOB estimate of  error rate: 8.68%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 799   2  14   0   5   0  0.02560976
2   1 363   1   1   5   3  0.02941176
3   2   1 718  11   0   7  0.02841678
4   4   2  64 202   2  49  0.37461300
5  16   2   1   3 319  27  0.13315217
7   0   1  12  45  17 736  0.09247842

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   1   2   5   0
  2   0 103   0   0   2   0
  3   3   0 214  19   0   5
  4   0   0   5  53   0   7
  5   3   0   0   0  85   4
  7   0   2   2  18  10 211


Misclassification Rate =  0.088

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.07612 0.09099 0.09711 0.09606 0.10236 0.11111 



mtry = 15:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=15,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 15,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 15

        OOB estimate of  error rate: 8.94%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 797   1  13   0   9   0  0.02804878
2   1 362   0   2   7   2  0.03208556
3   4   1 716  12   1   5  0.03112314
4   5   2  68 193   1  54  0.40247678
5  15   2   2   2 324  23  0.11956522
7   0   1  13  46  15 736  0.09247842

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   1   2   5   0
  2   0 103   0   0   2   0
  3   3   0 215  19   0   4
  4   0   0   4  51   0   8
  5   3   0   0   0  85   4
  7   0   2   2  20  10 211


Misclassification Rate =  0.089

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08136 0.09099 0.09624 0.09683 0.10149 0.11899 


mtry = 20:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=20,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 20,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 20

        OOB estimate of  error rate: 9%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 793   1  14   0  12   0  0.03292683
2   1 364   1   1   5   2  0.02673797
3   4   1 714  13   0   7  0.03382950
4   5   1  65 193   1  58  0.40247678
5  15   3   2   2 323  23  0.12228261
7   0   1  15  42  14 739  0.08877928

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 245   0   1   2   5   0
  2   0 103   0   0   1   0
  3   3   0 215  19   0   4
  4   0   0   4  51   0  10
  5   4   0   0   0  86   3
  7   0   2   2  20  10 210


Misclassification Rate =  0.09

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08136 0.09274 0.09886 0.09753 0.10411 0.10849 


mtry = 25:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=25,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 25,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 25

        OOB estimate of  error rate: 9.08%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 797   1  13   0   9   0  0.02804878
2   1 363   1   2   5   2  0.02941176
3   5   1 712  14   0   7  0.03653586
4   8   1  66 189   3  56  0.41486068
5  16   4   1   3 322  22  0.12500000
7   0   1  16  38  16 740  0.08754624

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 243   0   1   2   5   0
  2   0 102   0   0   1   0
  3   3   0 215  19   0   4
  4   0   0   5  50   0   9
  5   6   1   0   0  86   4
  7   0   2   1  21  10 210


Misclassification Rate =  0.094

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.07612 0.09186 0.09711 0.09890 0.10761 0.12161 


mtry = 30:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=30,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 30,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 30

        OOB estimate of  error rate: 9.2%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 800   1  10   0   9   0  0.02439024
2   1 362   1   0   8   2  0.03208556
3   7   1 712  13   0   6  0.03653586
4   8   2  67 187   1  58  0.42105263
5  19   4   0   5 317  23  0.13858696
7   0   1  17  37  15 741  0.08631319

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 242   0   1   1   5   0
  2   0 102   0   0   1   0
  3   3   0 214  20   0   4
  4   0   0   4  50   0  11
  5   7   1   0   0  86   4
  7   0   2   3  21  10 208


Misclassification Rate =  0.098

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08311 0.09711 0.10324 0.10268 0.11024 0.12248 


mtry = 36:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=36,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 36,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 36

        OOB estimate of  error rate: 9.46%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 799   1  10   0  10   0  0.02560976
2   2 360   1   1   7   3  0.03743316
3   6   0 710  13   0  10  0.03924222
4   6   1  66 183   3  64  0.43343653
5  17   4   1   3 321  22  0.12771739
7   0   0  17  42  15 737  0.09124538

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 242   0   1   1   5   0
  2   0 102   0   0   1   0
  3   3   0 214  21   0   5
  4   0   1   4  51   0  10
  5   7   1   0   0  86   4
  7   0   1   3  19  10 208


Misclassification Rate =  0.097

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.07962 0.09449 0.10236 0.10096 0.10849 0.11811 



Our answer probably is between 7 and 10


mtry = 8:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=8,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 8,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 8

        OOB estimate of  error rate: 8.82%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 797   2  16   0   5   0  0.02804878
2   1 364   1   1   4   3  0.02673797
3   2   1 717  12   0   7  0.02976996
4   3   2  67 196   2  53  0.39318885
5  19   2   0   3 320  24  0.13043478
7   0   1  14  43  15 738  0.09001233

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   1   2   5   0
  2   0 102   0   0   2   0
  3   3   0 215  19   0   5
  4   0   0   5  53   0   6
  5   3   1   0   0  85   4
  7   0   2   1  18  10 212


Misclassification Rate =  0.087

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08311 0.09011 0.09449 0.09718 0.10411 0.11461 


mtry = 9:
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=9,importance=T)
# sat.rf

```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 9,      importance = T) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 9

        OOB estimate of  error rate: 8.94%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 797   2  16   0   5   0  0.02804878
2   1 363   1   2   5   2  0.02941176
3   3   1 716  13   0   6  0.03112314
4   5   3  66 191   2  56  0.40866873
5  12   2   1   3 323  27  0.12228261
7   0   1  12  45  15 738  0.09001233

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   1   2   5   0
  2   0 103   0   0   2   0
  3   3   0 214  19   0   4
  4   0   0   5  52   0   6
  5   2   0   0   0  85   4
  7   0   2   2  19  10 213


Misclassification Rate =  0.086

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)

```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08224 0.09274 0.10061 0.09935 0.10586 0.11286 


Funnily enough, it turns out that 7 was actually the optimal value for mtry.

While the best model yet by far, we still can try to play around with a larger nodesize to bring that missclassification rate down further.


```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 2)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 2) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 9%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 800   2  14   0   4   0  0.02439024
2   1 364   1   2   4   2  0.02673797
3   2   1 718  11   0   7  0.02841678
4   5   3  65 193   2  55  0.40247678
5  22   2   0   3 317  24  0.13858696
7   0   0  14  47  16 734  0.09494451


```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 246   0   2   2   5   0
  2   0 103   0   0   2   0
  3   3   0 214  18   0   5
  4   0   0   5  54   0   8
  5   3   0   0   0  85   3
  7   0   2   1  18  10 211


Misclassification Rate =  0.087
```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08224 0.09274 0.09624 0.09760 0.10236 0.11811


We got ever so slightly worse on the test set, more nodesize likely isn't going to help us. Next we will try to see if more trees will provide us a gain or only overfit.



```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 750)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 1, ntree = 750) 
               Type of random forest: classification
                     Number of trees: 750
No. of variables tried at each split: 7

        OOB estimate of  error rate: 8.88%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 801   2  12   0   5   0  0.02317073
2   0 365   0   1   7   1  0.02406417
3   3   1 716  11   0   8  0.03112314
4   4   2  63 194   2  58  0.39938080
5  16   2   1   5 319  25  0.13315217
7   0   1  17  43  15 735  0.09371147

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   1   2   5   0
  2   0 103   0   0   2   0
  3   3   0 214  19   0   5
  4   0   0   5  52   0   6
  5   2   0   0   0  85   3
  7   0   2   2  19  10 213


Misclassification Rate =  0.086

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08399 0.09099 0.09711 0.09662 0.10061 0.11286 

More trees actually made a slightly worse model, likely due to more overfitting. Maybe if we cut it down a bit we can see something. Although this may not help due to randomforests needing a lot of trees.




```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 400)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 1, ntree = 400) 
               Type of random forest: classification
                     Number of trees: 400
No. of variables tried at each split: 7

        OOB estimate of  error rate: 8.79%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 798   2  14   0   6   0  0.02682927
2   0 363   0   2   5   4  0.02941176
3   3   1 717  11   0   7  0.02976996
4   3   3  65 195   3  54  0.39628483
5  15   2   1   2 321  27  0.12771739
7   0   1  13  46  12 739  0.08877928

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   1   2   5   0
  2   0 103   0   0   2   0
  3   3   0 215  19   0   5
  4   0   0   5  53   0   8
  5   2   0   0   0  85   3
  7   0   2   1  18  10 211


Misclassification Rate =  0.086

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08399 0.09186 0.09886 0.09708 0.10149 0.10849 


Reducing the nodesize again only caused it to diminish slightly. What if we set a limit to maximal node size?


Maxnodes = 10
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 500, maxnodes = 10)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 1, ntree = 500, maxnodes = 10) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 20.2%
Confusion matrix:
    1   2   3  4   5   7 class.error
1 803   0  13  0   1   3  0.02073171
2  40 330   0  0   1   3  0.11764706
3  44   0 690  2   0   3  0.06630582
4  98   0  80 46   0  99  0.85758514
5 130   7   0  0 182  49  0.50543478
7  81   0  19 18   3 690  0.14919852

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247  12  13  31  31  23
  2   0  91   0   0   3   0
  3   3   0 209  24   0   3
  4   0   0   0  12   0   4
  5   2   0   0   0  49   1
  7   0   2   0  25  19 196


Misclassification Rate =  0.196

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.07787 0.08924 0.09361 0.09480 0.09886 0.11286 

Maxnodes = 50
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 500, maxnodes = 50)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 1, ntree = 500, maxnodes = 50) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 12.31%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 802   1  15   0   2   0  0.02195122
2   5 353   1   5   5   5  0.05614973
3   6   0 713  13   1   6  0.03518268
4  15   1  68 171   0  68  0.47058824
5  59   1   0   3 260  45  0.29347826
7   4   0  18  71   5 713  0.12083847
```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   3  12   1
  2   0 102   0   0   2   0
  3   3   0 213  20   0   6
  4   0   0   5  47   0  16
  5   2   1   0   0  75   2
  7   0   2   2  22  13 202


Misclassification Rate =  0.114

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08136 0.08749 0.09536 0.09613 0.10411 0.11374 

Maxnodes = 100
```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 500, maxnodes = 100)
# sat.rf
```
Call:
 randomForest(formula = class ~ ., data = SATtrain, mtry = 7,      importance = T, nodesize = 1, ntree = 500, maxnodes = 100) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 7

        OOB estimate of  error rate: 11.21%
Confusion matrix:
    1   2   3   4   5   7 class.error
1 800   1  15   0   4   0  0.02439024
2   1 359   2   3   5   4  0.04010695
3   4   1 713  12   1   8  0.03518268
4  11   2  68 172   1  69  0.46749226
5  45   2   1   3 281  36  0.23641304
7   1   0  17  60   8 725  0.10604192

```{r}
# yhat = predict(sat.rf,newdata=SATtest)
# misclass(yhat,SATtest$class)

```
Table of Misclassification
(row = predicted, col = actual)
   y
fit   1   2   3   4   5   7
  1 247   0   2   2   9   0
  2   0 102   0   0   2   0
  3   3   0 213  19   0   5
  4   0   0   5  49   0  12
  5   2   1   0   0  79   3
  7   0   2   2  22  12 207


Misclassification Rate =  0.103

```{r}
# results = crf.sscv(sat.rf,SATtrain$class,data=SATtrain)
# summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.07612 0.09099 0.09711 0.09599 0.10149 0.11199

Trying to limit the algorithm manually begins to become counterintuitive, and beyond and endless brute force method, it is likely we will see only marginal improvement of dubious quality.


In the end, a realtively defualt model with only a specified mty of 7 variables was used to great effect over our previous models.

Final Model:

```{r}
# sat.rf = randomForest(class~.,data=SATtrain,mtry=7,importance=T, nodesize = 1, ntree = 500, maxnodes = 100)
```

Metrics:

Test Set: 0.084

Training Set (Mean):   0.098


Finally, we will build a boosted tree for classification.

Boosted Tree:

Packages:
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
require(adabag)
```




